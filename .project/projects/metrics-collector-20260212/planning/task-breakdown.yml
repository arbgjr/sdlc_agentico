---
# Task Breakdown - GitHub Metrics Collector
# REVISED: Local-First Architecture (SQLite + Task Scheduler + OneDrive)
# Generated by: delivery-planner
# Phase: 4 (Planning)
# Date: 2026-02-12T21:50:00Z

project:
  id: "metrics-collector-20260212"
  name: "GitHub Metrics Collector"
  complexity_level: 2
  architecture: "local-first"

planning_summary:
  total_sprints: 4
  sprint_duration_days: 14
  total_story_points: 98
  estimated_velocity: 25
  start_date: "2026-02-17"
  estimated_end_date: "2026-04-11"

sprints:

  # ========== SPRINT 1: Foundation + Core Collection ==========
  - sprint_number: 1
    name: "Foundation + Core Collection"
    goal: "Setup local infrastructure and basic Copilot collection"
    start_date: "2026-02-17"
    end_date: "2026-03-02"
    story_points: 28

    user_stories:
      - US-012  # Configurar Banco de Dados (SQLite)
      - US-001  # Coletar Metricas Copilot de Organizacao
      - US-002  # Coletar Metricas Copilot por Usuario
      - US-017  # Coletar Metricas de Requests Premium
      - US-018  # Retry com Backoff Exponencial
      - US-019  # Exportar CSVs com GZIP

    tasks:
      # Infrastructure Setup (Local)
      - id: "TASK-001"
        title: "Criar estrutura de projeto Python"
        story: "US-012"
        description: |
          Criar estrutura de diretorios e arquivos base:
          - src/
            - collectors/
            - calculators/
            - storage/
            - export/
            - utils/
          - data/
            - output/
            - export/
            - backup/
            - logs/
          - config/
          - tests/
          - requirements.txt
          - collector.py (main)
        acceptance_criteria:
          - "Estrutura de diretorios criada"
          - "requirements.txt com dependencias minimas"
          - "main.py executavel"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["infra", "setup", "phase:5"]

      - id: "TASK-002"
        title: "Criar schema SQLite e StorageManager"
        story: "US-012"
        description: |
          Criar schema SQLite conforme data-model.md:
          - Tabelas: organizations, repositories, copilot_*, dora_*, velocity_*, collection_runs
          - Indices para consultas frequentes
          - StorageManager com dual storage (JSON + SQLite)
        acceptance_criteria:
          - "Schema criado via migration script"
          - "StorageManager persiste em JSON e SQLite"
          - "Queries funcionando"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["database", "sqlite", "phase:5"]

      - id: "TASK-003"
        title: "Implementar retry decorator com backoff"
        story: "US-018"
        description: |
          Criar decorator @retry para chamadas de API:
          - Max 3 tentativas
          - Backoff exponencial (60s, 120s, 240s)
          - Nao retry em erros 401/403
          - Log de cada tentativa
        acceptance_criteria:
          - "Decorator funcionando"
          - "Backoff exponencial correto"
          - "Erros de auth nao fazem retry"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["utils", "retry", "phase:5"]

      - id: "TASK-004"
        title: "Implementar cliente GitHub API"
        story: "US-001"
        description: |
          Criar modulo Python para interacao com GitHub API:
          - Autenticacao via PAT (env variable)
          - Rate limit handling (429, Retry-After)
          - Usar retry decorator
          - Logging estruturado
        acceptance_criteria:
          - "Cliente funcional para REST API"
          - "Rate limit respeitado"
          - "Logs estruturados com timestamps"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["api", "github", "phase:5"]

      - id: "TASK-005"
        title: "Implementar CopilotMetricsCollector"
        story: "US-001"
        description: |
          Implementar coleta de metricas Copilot:
          - GET /enterprises/{enterprise}/copilot/usage
          - Parsear NDJSON response
          - Salvar JSON raw + SQLite (dual storage)
        acceptance_criteria:
          - "Coleta funcionando para 2 enterprises"
          - "Dados salvos em JSON e SQLite"
          - "Privacy threshold tratado"
        estimate_hours: 5
        story_points: 4
        assignee: "code-author"
        labels: ["copilot", "api", "phase:5"]

      - id: "TASK-006"
        title: "Implementar coleta per-user Copilot"
        story: "US-002"
        description: |
          Estender CopilotMetricsCollector para nivel usuario:
          - Extrair breakdown por usuario
          - Armazenar em copilot_user_metrics
          - Manter github_user_id como chave imutavel
        acceptance_criteria:
          - "Metricas por usuario armazenadas"
          - "Usuarios desativados mantidos no historico"
        estimate_hours: 3
        story_points: 2
        assignee: "code-author"
        labels: ["copilot", "per-user", "phase:5"]

      - id: "TASK-007"
        title: "Implementar PremiumRequestsCollector"
        story: "US-017"
        description: |
          Implementar coleta de requests premium vs standard:
          - Verificar endpoint disponivel (usage ou billing)
          - Extrair premium_requests_count, standard_requests_count
          - Extrair total_tokens_consumed, model_used
          - Salvar em copilot_premium_requests
        acceptance_criteria:
          - "Metricas premium vs standard coletadas"
          - "Modelo usado identificado"
          - "Fallback graceful se API nao suportar"
        estimate_hours: 5
        story_points: 4
        assignee: "code-author"
        labels: ["copilot", "premium", "phase:5"]

      - id: "TASK-008"
        title: "Implementar CSVExporter com GZIP"
        story: "US-019"
        description: |
          Criar modulo de export CSV comprimido:
          - UTF-8 BOM encoding
          - Semicolon delimiter
          - GZIP compression (.csv.gz)
          - Gerar 4 arquivos: copilot, premium, dora, velocity
        acceptance_criteria:
          - "CSV.GZ gerados corretamente"
          - "Tamanho ~80% menor que CSV puro"
          - "PowerBI importa sem erros"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["export", "csv", "gzip", "phase:5"]

      - id: "TASK-009"
        title: "Implementar OneDriveCopier"
        story: "US-019"
        description: |
          Criar modulo para copiar arquivos para OneDrive:
          - Usar shutil.copy2 (preserva metadata)
          - Configurar path OneDrive via config
          - Verificar copia bem sucedida
        acceptance_criteria:
          - "Arquivos copiados para OneDrive"
          - "Erro se path nao existe"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["onedrive", "export", "phase:5"]

      - id: "TASK-010"
        title: "Criar configuracao via YAML"
        story: "US-012"
        description: |
          Criar config/config.yml com:
          - Enterprises e PAT env vars
          - Paths (data, onedrive)
          - Retencao (JSON 90d, logs 30d)
          - Horarios (coleta, backup)
        acceptance_criteria:
          - "Config carregado no startup"
          - "Validacao de config obrigatorias"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["config", "setup", "phase:5"]

      - id: "TASK-011"
        title: "Criar logging estruturado"
        story: "US-012"
        description: |
          Configurar logging para arquivo e console:
          - Arquivo: data/logs/collector_YYYYMMDD.log
          - Formato: timestamp | level | module | message
          - Log rotation manual (cleanup antigos)
        acceptance_criteria:
          - "Logs gravados em arquivo"
          - "Formato legivel e parseable"
        estimate_hours: 2
        story_points: 1
        assignee: "code-author"
        labels: ["logging", "observability", "phase:5"]

  # ========== SPRINT 2: DORA & Velocity + Backup ==========
  - sprint_number: 2
    name: "DORA & Velocity + Backup"
    goal: "Implement DORA/Velocity calculations and backup system"
    start_date: "2026-03-03"
    end_date: "2026-03-16"
    story_points: 28

    user_stories:
      - US-004  # Deployment Frequency
      - US-005  # Lead Time
      - US-006  # Change Failure Rate
      - US-008  # Commit Frequency
      - US-009  # PR Throughput
      - US-020  # Cleanup Automatico
      - US-022  # Backup SQLite

    tasks:
      - id: "TASK-012"
        title: "Implementar GitActivityCollector"
        story: "US-004"
        description: |
          Coletar dados de workflows, PRs e commits:
          - GET /repos/{owner}/{repo}/actions/runs
          - GET /repos/{owner}/{repo}/pulls?state=closed
          - GET /repos/{owner}/{repo}/commits
          - Salvar em commits e pull_requests tables
        acceptance_criteria:
          - "Workflow runs coletados"
          - "PRs merged identificados"
          - "Commits salvos com author"
        estimate_hours: 6
        story_points: 5
        assignee: "code-author"
        labels: ["dora", "velocity", "api", "phase:5"]

      - id: "TASK-013"
        title: "Implementar DORACalculator"
        story: "US-004"
        description: |
          Calcular metricas DORA via SQL no SQLite:
          - Deployment Frequency (deploys/week)
          - Lead Time (first commit to deploy)
          - Change Failure Rate (failed/total)
          - Classificacao DORA (Elite/High/Medium/Low)
        acceptance_criteria:
          - "Todas 4 metricas DORA calculadas"
          - "Classificacao aplicada corretamente"
        estimate_hours: 6
        story_points: 5
        assignee: "code-author"
        labels: ["dora", "calculation", "sql", "phase:5"]

      - id: "TASK-014"
        title: "Calcular Lead Time for Changes"
        story: "US-005"
        description: |
          Implementar calculo de lead time:
          - First commit to deploy time
          - Media, P50, P95 via SQL
          - Classificacao DORA
        acceptance_criteria:
          - "Lead time calculado corretamente"
          - "Percentis usando SQLite window functions"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["dora", "lead-time", "phase:5"]

      - id: "TASK-015"
        title: "Calcular Change Failure Rate"
        story: "US-006"
        description: |
          Implementar calculo de CFR:
          - Detectar rollbacks via workflow name
          - Detectar issues com label incident/hotfix
          - Calcular failed/total
        acceptance_criteria:
          - "CFR calculado"
          - "Rollbacks detectados"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["dora", "cfr", "phase:5"]

      - id: "TASK-016"
        title: "Implementar VelocityCalculator"
        story: "US-008"
        description: |
          Calcular metricas de velocity via SQL:
          - Commits per day per developer
          - PRs merged per week
          - Excluir bots automaticamente
        acceptance_criteria:
          - "Commit frequency calculada"
          - "PR throughput calculado"
          - "Bots filtrados"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["velocity", "calculation", "phase:5"]

      - id: "TASK-017"
        title: "Calcular PR Throughput"
        story: "US-009"
        description: |
          Implementar calculo de PR throughput:
          - PRs merged per week per author
          - PRs merged per week per repository
          - Excluir PRs fechados sem merge
        acceptance_criteria:
          - "Throughput calculado"
          - "PRs closed-not-merged excluidos"
        estimate_hours: 3
        story_points: 2
        assignee: "code-author"
        labels: ["velocity", "pr", "phase:5"]

      - id: "TASK-018"
        title: "Implementar BackupManager"
        story: "US-022"
        description: |
          Criar sistema de backup automatico:
          - backup_sqlite(): copia para OneDrive diariamente
          - dump_sql(): gera .sql.gz semanalmente
          - Limpar backups antigos
        acceptance_criteria:
          - "Backup diario funcionando"
          - "SQL dump semanal funcionando"
          - "Backups antigos removidos"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["backup", "maintenance", "phase:5"]

      - id: "TASK-019"
        title: "Implementar CleanupManager"
        story: "US-020"
        description: |
          Criar sistema de cleanup automatico:
          - JSON files > 90 dias: delete
          - Log files > 30 dias: delete
          - SQLite backups locais > 7 dias: delete
        acceptance_criteria:
          - "Cleanup executando apos coleta"
          - "Arquivos removidos logados"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["cleanup", "maintenance", "phase:5"]

      - id: "TASK-020"
        title: "Criar Task Scheduler para SQL dump"
        story: "US-022"
        description: |
          Configurar Task Scheduler separado para dump semanal:
          - Nome: "GitHub Metrics SQL Dump"
          - Trigger: Weekly, Sunday 23:00
          - Action: python dump_sql.py
        acceptance_criteria:
          - "Task Scheduler configurado"
          - "Dump executando no domingo"
        estimate_hours: 1
        story_points: 1
        assignee: "iac-engineer"
        labels: ["scheduling", "backup", "phase:5"]

  # ========== SPRINT 3: Scheduling + Polish ==========
  - sprint_number: 3
    name: "Scheduling + Polish"
    goal: "Task Scheduler setup, review time, MTTR, alertas"
    start_date: "2026-03-17"
    end_date: "2026-03-30"
    story_points: 24

    user_stories:
      - US-003  # Copilot por Linguagem
      - US-007  # MTTR
      - US-010  # Review Time
      - US-015  # Scheduling Automatico
      - US-021  # Alertas via Power Automate

    tasks:
      - id: "TASK-021"
        title: "Implementar coleta por linguagem"
        story: "US-003"
        description: |
          Extrair breakdown por linguagem do Copilot:
          - Parsear language breakdown do response
          - Armazenar em campo JSONB ou tabela separada
          - Mapear linguagens desconhecidas para 'Other'
        acceptance_criteria:
          - "Breakdown por linguagem disponivel"
          - "Linguagens nao mapeadas tratadas"
        estimate_hours: 3
        story_points: 2
        assignee: "code-author"
        labels: ["copilot", "language", "phase:5"]

      - id: "TASK-022"
        title: "Estimar MTTR"
        story: "US-007"
        description: |
          Implementar estimativa de MTTR:
          - Buscar issues com label 'incident'
          - Calcular tempo open-to-closed
          - Media e classificacao DORA
        acceptance_criteria:
          - "MTTR estimado quando ha incidentes"
          - "Retorna null quando nao ha incidentes"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["dora", "mttr", "phase:5"]

      - id: "TASK-023"
        title: "Calcular Review Time"
        story: "US-010"
        description: |
          Implementar calculo de review time:
          - GET /repos/{owner}/{repo}/pulls/{n}/reviews
          - Time to first review
          - Media, P50, P95 via SQL
        acceptance_criteria:
          - "Review time calculado"
          - "Self-merges excluidos"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["velocity", "review", "phase:5"]

      - id: "TASK-024"
        title: "Criar collector.py principal"
        story: "US-015"
        description: |
          Criar script principal que orquestra:
          1. Carregar config
          2. Executar CopilotMetricsCollector
          3. Executar PremiumRequestsCollector
          4. Executar GitActivityCollector
          5. Executar DORACalculator
          6. Executar VelocityCalculator
          7. Executar CSVExporter
          8. Executar OneDriveCopier
          9. Executar BackupManager
          10. Executar CleanupManager
          11. Registrar em collection_runs
        acceptance_criteria:
          - "Coleta completa de ponta a ponta"
          - "Erros nao param toda a coleta"
          - "Status final registrado"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["orchestration", "main", "phase:5"]

      - id: "TASK-025"
        title: "Configurar Windows Task Scheduler"
        story: "US-015"
        description: |
          Configurar Task Scheduler para coleta diaria:
          - Nome: "GitHub Metrics Collector"
          - Trigger: Daily at 8:00 AM
          - Action: python collector.py
          - Working directory configurado
          - Start when available = true
        acceptance_criteria:
          - "Task Scheduler executando"
          - "Logs sendo gerados"
        estimate_hours: 2
        story_points: 2
        assignee: "iac-engineer"
        labels: ["scheduling", "infra", "phase:5"]

      - id: "TASK-026"
        title: "Implementar AlertManager"
        story: "US-021"
        description: |
          Criar sistema de alertas:
          - Detectar 3+ falhas consecutivas
          - Gerar arquivo trigger para Power Automate
          - Incluir: data/hora, erro, ultimas tentativas
        acceptance_criteria:
          - "Alerta gerado apos 3 falhas"
          - "Arquivo trigger criado em OneDrive"
        estimate_hours: 3
        story_points: 3
        assignee: "code-author"
        labels: ["alerts", "monitoring", "phase:5"]

      - id: "TASK-027"
        title: "Configurar Power Automate flow"
        story: "US-021"
        description: |
          Configurar Power Automate para alertas:
          - Trigger: novo arquivo em OneDrive/alerts/
          - Action: enviar email com conteudo do arquivo
          - Destinatario configuravel
        acceptance_criteria:
          - "Flow criado e testado"
          - "Email recebido quando alerta gerado"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["alerts", "power-automate", "phase:5"]

      - id: "TASK-028"
        title: "Configurar Power Automate para PowerBI"
        story: "US-015"
        description: |
          Configurar Power Automate para trigger de refresh:
          - Trigger: novo arquivo em OneDrive/exports/
          - Action: trigger PowerBI dataset refresh
        acceptance_criteria:
          - "Flow criado"
          - "Refresh disparado quando CSV chega"
        estimate_hours: 2
        story_points: 2
        assignee: "code-author"
        labels: ["powerbi", "power-automate", "phase:5"]

      - id: "TASK-029"
        title: "Criar testes unitarios core"
        story: null
        description: |
          Criar testes unitarios para modulos criticos:
          - test_retry_decorator.py
          - test_storage_manager.py
          - test_csv_exporter.py
          - test_dora_calculator.py
        acceptance_criteria:
          - "Cobertura > 80% dos modulos criticos"
          - "Testes passando"
        estimate_hours: 6
        story_points: 4
        assignee: "test-author"
        labels: ["testing", "quality", "phase:6"]

  # ========== SPRINT 4: Historical + Documentation ==========
  - sprint_number: 4
    name: "Historical + Documentation"
    goal: "Historical import, code churn, operational docs"
    start_date: "2026-03-31"
    end_date: "2026-04-11"
    story_points: 18

    user_stories:
      - US-011  # Code Churn
      - US-016  # Importar Dados Historicos
      - US-014  # Configurar Conexao PowerBI

    tasks:
      - id: "TASK-030"
        title: "Calcular Code Churn"
        story: "US-011"
        description: |
          Implementar calculo de code churn:
          - Lines added, lines deleted por commit
          - Churn rate = (added + deleted) / total
          - Excluir arquivos binarios
        acceptance_criteria:
          - "Churn calculado"
          - "Binarios excluidos"
        estimate_hours: 3
        story_points: 3
        assignee: "code-author"
        labels: ["velocity", "churn", "phase:5"]

      - id: "TASK-031"
        title: "Criar script de importacao historica"
        story: "US-016"
        description: |
          Criar script para importar dados historicos do ghc_metrics:
          - Aceitar JSON como input
          - Validar schema
          - Detectar e ignorar duplicatas
          - Relatorio de erros
        acceptance_criteria:
          - "Importacao funcional"
          - "Duplicatas detectadas"
          - "Relatorio de erros gerado"
        estimate_hours: 6
        story_points: 5
        assignee: "code-author"
        labels: ["historical", "import", "phase:5"]

      - id: "TASK-032"
        title: "Importar dados historicos 2023-2026"
        story: "US-016"
        description: |
          Executar importacao dos dados historicos:
          - Coletar JSON files do ghc_metrics existente
          - Executar script de importacao
          - Validar dados importados
        acceptance_criteria:
          - "Dados desde 2023 importados"
          - "Zero duplicatas"
          - "Validacao manual ok"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["historical", "migration", "phase:5"]

      - id: "TASK-033"
        title: "Criar template PowerBI"
        story: "US-014"
        description: |
          Criar arquivo .pbit com:
          - Conexao configurada para CSV.GZ do OneDrive
          - Tabelas: copilot, premium, dora, velocity
          - Relacionamentos basicos
          - Medidas DAX essenciais
        acceptance_criteria:
          - "Template importa dados corretamente"
          - "Relacionamentos funcionando"
        estimate_hours: 4
        story_points: 3
        assignee: "code-author"
        labels: ["powerbi", "template", "phase:5"]

      - id: "TASK-034"
        title: "Criar documentacao operacional"
        story: null
        description: |
          Criar documentacao para operacao:
          - README.md com setup instructions
          - docs/runbook.md com troubleshooting
          - docs/pat-rotation.md com procedimento
          - docs/new-enterprise.md com guia
        acceptance_criteria:
          - "Documentacao completa"
          - "Procedimentos testados"
        estimate_hours: 4
        story_points: 2
        assignee: "doc-generator"
        labels: ["documentation", "runbook", "phase:7"]

      - id: "TASK-035"
        title: "Teste de integracao end-to-end"
        story: null
        description: |
          Executar teste completo do sistema:
          - Executar coleta manual
          - Verificar todos os arquivos gerados
          - Verificar copia para OneDrive
          - Verificar import no PowerBI
          - Verificar backup funcionando
        acceptance_criteria:
          - "Sistema funcionando end-to-end"
          - "Todos os arquivos corretos"
          - "PowerBI com dados"
        estimate_hours: 4
        story_points: 2
        assignee: "qa-analyst"
        labels: ["testing", "e2e", "phase:6"]

# Summary
summary:
  total_tasks: 35
  total_story_points: 98
  by_sprint:
    sprint_1:
      tasks: 11
      story_points: 28
      focus: "Infrastructure local + Copilot collection"
    sprint_2:
      tasks: 9
      story_points: 28
      focus: "DORA + Velocity + Backup"
    sprint_3:
      tasks: 9
      story_points: 24
      focus: "Scheduling + Alertas + Testes"
    sprint_4:
      tasks: 6
      story_points: 18
      focus: "Historical + PowerBI + Docs"

  architecture_summary:
    database: "SQLite local (data/metrics.db)"
    scheduler: "Windows Task Scheduler (8:00 AM)"
    storage: "OneDrive (CSV.GZ + backups)"
    cost: "$0/month"

  critical_path:
    - "TASK-001"  # Estrutura projeto
    - "TASK-002"  # Schema SQLite
    - "TASK-004"  # GitHub client
    - "TASK-005"  # Copilot collector
    - "TASK-012"  # Git activity collector
    - "TASK-013"  # DORA calculator
    - "TASK-024"  # collector.py main
    - "TASK-025"  # Task Scheduler

  risks:
    - risk: "GitHub PATs nao obtidos a tempo"
      impact: "high"
      mitigation: "Solicitar PATs antes do Sprint 1"

    - risk: "Copilot API nao retorna breakdown premium"
      impact: "medium"
      mitigation: "Implementar fallback graceful"

    - risk: "Maquina desligada durante coleta"
      impact: "medium"
      mitigation: "Start when available = true no Task Scheduler"

    - risk: "OneDrive path nao acessivel"
      impact: "medium"
      mitigation: "Validar path no startup, alertar se falhar"

  dependencies_external:
    - dependency: "GitHub PAT Enterprise 1"
      owner: "GitHub Admin"
      due_date: "2026-02-14"
      status: "pending"

    - dependency: "GitHub PAT Enterprise 2"
      owner: "GitHub Admin"
      due_date: "2026-02-14"
      status: "pending"

    - dependency: "OneDrive folder access"
      owner: "User (already available)"
      due_date: "N/A"
      status: "available"

    - dependency: "PowerBI workspace"
      owner: "BI Team"
      due_date: "2026-02-21"
      status: "available"

  changelog:
    - date: "2026-02-12T21:50:00Z"
      change: "Rewritten for local-first architecture"
      reason: "User rejected cloud architecture (Azure Functions, PostgreSQL)"
