id: ADR-020
type: decision
title: "Multi-Model Configuration System"
status: accepted
date: "2026-01-23"
supersedes: null
deprecated_by: null

context: |
  O SDLC Agêntico possui 36 agentes especializados que fazem chamadas à API da Anthropic.
  Atualmente, a seleção de modelo é hardcoded no frontmatter de cada agente (opus ou sonnet),
  resultando em vários problemas:

  Problema 1: Uso ineficiente de recursos - tarefas simples usam modelos caros
  Problema 2: Sem auto-upgrade quando conteúdo excede limites do modelo
  Problema 3: Sem visibilidade ou otimização de custos
  Problema 4: Diferença de custo de 60x entre haiku e opus não é aproveitada
  Problema 5: Sem mecanismo de fallback quando modelo atinge limites de contexto
  Problema 6: Sem enforcement de budget constraints por projeto

  Custos atuais (por 1M tokens):
  - haiku input: $0.25, output: $1.25
  - sonnet input: $3.00, output: $15.00
  - opus input: $15.00, output: $75.00

decision: |
  Implementar sistema hierárquico de configuração de modelos que:

  1. CONFIGURATION: Schema YAML declarativo com regras de seleção por fase/agente/tipo
  2. SELECTION: Biblioteca Python que seleciona modelo ideal baseado em contexto
  3. AUTO-UPGRADE: Detecção automática quando conteúdo excede threshold
  4. COST TRACKING: Logging estruturado de uso e custos para Loki/Grafana
  5. BUDGET ENFORCEMENT: Limites diários/mensais configuráveis
  6. QUALITY GATE: Validação de configuração antes de deployment

  Arquitetura:
  - model-strategy.yml: Configuração declarativa (git-tracked)
  - model_selector.py: Lógica de seleção e cost estimation
  - Integração via hook no settings.json (transparente para agentes)
  - Dashboard Grafana para visibilidade de custos

alternatives:
  - option: "Opção 1: Hardcoded model selection (status quo)"
    pros:
      - "Simplicidade - sem configuração adicional"
      - "Previsível - sempre mesmo modelo"
    cons:
      - "Custos 40-60% maiores que otimizado"
      - "Sem adaptação a conteúdo grande"
      - "Sem visibilidade de custos"
    rejected_reason: "ROI negativo - payback em 4 meses com economia de $400/mês"

  - option: "Opção 2: ML-based dynamic selection"
    pros:
      - "Potencialmente mais preciso"
      - "Aprende padrões de uso ao longo do tempo"
    cons:
      - "Complexidade extrema (requer training data)"
      - "Black box - difícil de debugar"
      - "Overhead computacional"
      - "Não justificado para ~36 agentes"
    rejected_reason: "Over-engineering para escala atual, rule-based é suficiente"

  - option: "Opção 3: Configuração via environment variables apenas"
    pros:
      - "Simples de implementar"
      - "Não requer arquivo de config"
    cons:
      - "Não suporta regras complexas (overrides por agente)"
      - "Difícil de versionar e auditar"
      - "Não permite hierarquia de configuração"
    rejected_reason: "Não suporta casos de uso avançados (agent overrides)"

consequences:
  positive:
    - "Economia estimada de 40% em custos ($400/mês em projeto médio)"
    - "Auto-upgrade transparente para conteúdo grande"
    - "Visibilidade completa de custos via Grafana"
    - "Budget enforcement previne surpresas de faturamento"
    - "Backward compatible - configs existentes funcionam"
    - "Testável - 90%+ coverage em model_selector.py"

  negative:
    - "Overhead mínimo de seleção (~50ms por chamada)"
    - "Configuração adicional a ser mantida"
    - "Migração de 36 agentes necessária"

  neutral:
    - "Novo quality gate (model-selection-gate.yml)"
    - "Dashboard adicional no Grafana"

trade_offs:
  - aspect: "Simplicidade vs. Otimização"
    chosen: "Otimização com sane defaults"
    rationale: "Configuração default funciona out-of-box, otimizações opcionais"

  - aspect: "Performance vs. Cost Tracking"
    chosen: "Logging assíncrono para minimizar latência"
    rationale: "Overhead <50ms aceitável para economia de 40%"

  - aspect: "Flexibilidade vs. Complexidade"
    chosen: "Hierarquia de 3 níveis (default > phase > agent)"
    rationale: "Cobre 95% casos de uso sem over-engineering"

  - aspect: "Budget Enforcement Strictness"
    chosen: "Soft limits (warning) + hard limits (block)"
    rationale: "Previne bloqueio acidental em emergências"

implementation_notes:
  selection_hierarchy: |
    Ordem de precedência (maior para menor):
    1. Agent override (system-architect: opus)
    2. Task type override (documentation: haiku)
    3. Phase override (phase 3: opus)
    4. Default model (sonnet)

    Auto-upgrade triggers:
    - Input tokens > threshold → upgrade to next tier
    - haiku (10K) → sonnet (100K) → opus (200K)
    - Logged: model_upgrade_triggered=true

  configuration_schema: |
    model_strategy:
      default_model: sonnet  # Default para todos agentes
      content_thresholds:
        haiku: {max_input: 10000, max_output: 2048}
        sonnet: {max_input: 100000, max_output: 8192}
        opus: {max_input: 200000, max_output: 16384}
      cost_per_million:
        haiku: {input: 0.25, output: 1.25}
        sonnet: {input: 3.00, output: 15.00}
        opus: {input: 15.00, output: 75.00}
      by_phase:
        0: sonnet  # Intake
        3: opus    # Architecture (requires deep reasoning)
        8: opus    # Operations (incident analysis)
      by_task_type:
        documentation: haiku
        architecture: opus
        security: opus
      by_agent:
        threat-modeler: opus
        system-architect: opus
        compliance-guardian: opus
      auto_upgrade:
        enabled: true
        log_upgrades: true
      budget:
        daily_limit_usd: 100.00
        monthly_limit_usd: 2000.00
        soft_warning_threshold: 0.80  # Warning at 80%
        hard_block_threshold: 1.00    # Block at 100%

  cost_calculation: |
    def estimate_cost(model, input_tokens, output_tokens):
        pricing = config['cost_per_million'][model]
        input_cost = (input_tokens / 1_000_000) * pricing['input']
        output_cost = (output_tokens / 1_000_000) * pricing['output']
        return input_cost + output_cost

    Tracked metrics:
    - Total cost per day/month
    - Cost by model (haiku/sonnet/opus)
    - Cost by phase (0-8)
    - Cost by agent
    - Auto-upgrade frequency

  quality_gate: |
    model-selection-gate.yml valida:
    - CRITICAL: config_valid (YAML parseable)
    - CRITICAL: all_phases_covered (0-8 configurados)
    - CRITICAL: cost_tracking_enabled (Loki accessible)
    - WARNING: thresholds_logical (haiku < sonnet < opus)
    - WARNING: budget_limits_reasonable (> 0)

  error_handling: |
    - Config file missing → Use hardcoded defaults, log warning
    - Model API rate limit → Retry with exponential backoff (3x)
    - Budget exceeded → Log critical, optionally block (configurable)
    - Invalid model name → Fallback to sonnet, log error
    - Cost tracking failure → Continue execution, log error (non-blocking)

testing_coverage: |
  Testes unitários (target 90%+):
  - test_select_model.py: 15 tests (hierarchy, overrides, auto-upgrade)
  - test_estimate_cost.py: 8 tests (cost calculation accuracy)
  - test_check_budget.py: 6 tests (soft/hard limits, warnings)
  - test_integration.py: 5 tests (end-to-end com agentes reais)

  Integration tests:
  - Verificar modelo correto por cenário
  - Validar auto-upgrade em conteúdo grande
  - Confirmar cost tracking no Loki
  - Testar budget enforcement

scalability_considerations: |
  - Seleção de modelo: O(1) lookup em dict (< 1ms)
  - Cost tracking: Async logging para não bloquear chamadas
  - Configuração cacheada em memória (reload apenas se arquivo modificado)
  - Suporta até 1000 agentes sem degradação de performance

security_considerations: |
  - Configuração em YAML plain-text (sem secrets)
  - Budget limits previnem runaway costs (DoS financeiro)
  - Logging de todas seleções para auditoria
  - No API keys em config (usa environment variables)

dependencies:
  - sdlc_logging.py: Structured logging com Loki
  - PyYAML: Parsing de configuração
  - Grafana: Dashboards de custo

implementation_references:
  awesome_copilot_repo: |
    Repository: https://github.com/github/awesome-copilot

    IMPORTANTE: Ao criar skills, agents e dashboards, SEMPRE consultar
    awesome-copilot primeiro. O repositório já possui várias implementações prontas
    que funcionam e devem ser usadas como base e exemplos.

    Exemplos relevantes para Multi-Model Config:
    - Cost tracking patterns
    - Configuration management examples
    - Dashboard templates (Grafana)
    - Model selection heuristics
    - Budget enforcement scripts

  claude_plugins_official: |
    Repository: https://github.com/anthropics/claude-plugins-official

    IMPORTANTE: Repositório oficial de plugins do Claude/Anthropic. Consultar
    para padrões oficiais de implementação de skills, integrações e extensões.

    Exemplos relevantes para Multi-Model Config:
    - Plugin architecture patterns
    - Configuration management (official patterns)
    - Cost tracking integrations
    - Model selection APIs
    - Official Anthropic SDK usage examples
    - Best practices para Claude extensions

  workflow_recomendado: |
    1. Buscar em awesome-copilot por funcionalidade similar
    2. Buscar em claude-plugins-official por padrões oficiais
    3. Analisar implementações existentes em ambos repos
    4. Adaptar para seleção de modelos do SDLC Agêntico
    5. Citar fontes no código:
       # Adapted from awesome-copilot: <URL>
       # Based on claude-plugins-official: <URL>
    6. Contribuir melhorias de volta (se aplicável)

    Prioridade:
    - claude-plugins-official para padrões arquiteturais
    - awesome-copilot para implementações community-tested

  beneficios: |
    - Evita reinventar a roda
    - Aproveita battle-tested implementations (awesome-copilot)
    - Segue padrões oficiais Anthropic (claude-plugins-official)
    - Acelera desenvolvimento (~20-30% time saving)
    - Mantém compatibilidade com ecossistema Claude

migration_path: |
  Backward compatibility garantida:
  1. Agentes sem configuração usam default (sonnet)
  2. Agentes com hardcoded model continuam funcionando
  3. Migration script atualiza settings.json gradualmente
  4. Rollback via git revert (<15min RTO)

  Migration checklist:
  - [ ] Criar model-strategy.yml
  - [ ] Instalar model_selector.py
  - [ ] Atualizar settings.json (hook)
  - [ ] Atualizar 36 agentes (remover hardcoded model)
  - [ ] Rodar integration tests
  - [ ] Deploy Grafana dashboard
  - [ ] Monitor custos por 1 semana

rollback_strategy: |
  Se sistema causar issues:
  1. Restaurar settings.json.backup
  2. git revert <commit-hash>
  3. Validar agentes usam modelos hardcoded
  4. Verificar custos não aumentaram

  RTO: < 15 minutos
  RPO: 0 (sem perda de dados, apenas configuração)

future_enhancements:
  v2_0:
    - "Auto-tuning de thresholds baseado em histórico"
    - "Cost predictions (forecast semanal/mensal)"
    - "Alertas proativos antes de atingir budget"
  v2_1:
    - "Multi-provider support (Azure OpenAI, AWS Bedrock)"
    - "Model performance tracking (latência, accuracy)"
    - "Automatic model deprecation handling"
  v2_2:
    - "Cost allocation por projeto/cliente"
    - "Chargebacks automáticos"
    - "FinOps dashboard completo"

related_decisions:
  - "ADR-007: Structured Logging with Loki"
  - "ADR-claude-orchestrator-integration: Multi-agent workflows"

tags:
  - cost-optimization
  - model-selection
  - budget-management
  - observability
  - performance

validation:
  test_coverage: "Target 90%+ (TBD)"
  code_review: "Required before merge"
  quality_gate: "model-selection-gate.yml passing"
  integration: "Tested with all 36 agents"
